version: '2' 
services:
  namenode:
    image: bde2020/hadoop-namenode:1.0.0
    hostname: namenode
    container_name: namenode
    domainname: hadoop
    volumes:
      - ./data/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env
    ports:
      - "50070:50070"
      - "8020:8020"
    networks:
      - hadoop

  datanode1:
    image: bde2020/hadoop-datanode:1.0.0
    hostname: datanode1
    container_name: datanode1
    domainname: hadoop
    volumes:
      - ./data/datanode1:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
    networks:
      - hadoop

  datanode2:
    image: bde2020/hadoop-datanode:1.0.0
    hostname: datanode2
    container_name: datanode2
    domainname: hadoop
    volumes:
      - ./data/datanode2:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
    networks:
      - hadoop

  spark-master:
    image: thuongdinh/docker-spark-anaconda:latest
    hostname: spark-master
    command: bin/spark-class org.apache.spark.deploy.master.Master -h spark-master
    environment:
      - MASTER=spark://spark-master:7077
      - SPARK_PUBLIC_DNS=localhost
    volumes:
      - ./notebooks:/opt/notebooks
    container_name: spark-master
    domainname: hadoop
    ports:
        - "9090:8080"
    links:
      - namenode
    networks:
      - hadoop

  spark-worker:
    image: thuongdinh/docker-spark-anaconda:latest
    hostname: spark-worker
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    domainname: hadoop
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=1g
      - SPARK_PUBLIC_DNS=localhost
    volumes:
      - ./notebooks:/opt/notebooks
    links:
      - spark-master
    networks:
      - hadoop

  jupyter-notebook:
    image: thuongdinh/docker-spark-anaconda:latest
    hostname: jupyternotebook
    container_name: jupyternotebook
    domainname: hadoop
    command: /bin/bash -c "/opt/conda/bin/conda install jupyter -y --quiet && /opt/conda/bin/jupyter notebook --notebook-dir=/opt/notebooks --ip='*' --port=8888 --no-browser"
    env_file:
      - ./hadoop.env
    volumes:
      - ./notebooks:/opt/notebooks
    ports:
      - "8889:8888"
    links:
      - spark-master
    networks:
      - hadoop

  hue:
    image: bde2020/hdfs-filebrowser:3.9
    hostname: hdfsfb
    container_name: hdfsfb
    domainname: hadoop
    networks:
      - hadoop
    environment:
      - NAMENODE_HOST=namenode
    ports:
      - "8088:8088"

networks:
  hadoop:
    external: true
